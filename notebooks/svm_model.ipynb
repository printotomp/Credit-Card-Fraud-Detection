#  Imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# ────────────────────────────────────────────────
#  Manual Preprocessing and Feature Selection
# ────────────────────────────────────────────────
df = pd.read_csv("creditcard.csv", nrows=1000)  # Use None for full dataset
X = df.drop(columns=['Time', 'Amount', 'Class']).to_numpy()
y = df['Class'].copy().to_numpy()
y[y == 0] = -1  # Convert class labels to {-1, 1}

# ────────────────────────────────────────────────
#  Custom Hinge Loss, Forward Pass, Gradient Update
# ────────────────────────────────────────────────
def hinge_loss_fn(y, y_hat):
    return np.maximum(0, 1 - y * y_hat)

def forward(x, w):
    return np.dot(x, w)

def svm_gradient(x, y, w, epoch):
    if y * forward(x, w) < 1:
        return x * y - 2 * (1 / epoch) * w
    else:
        return -2 * (1 / epoch) * w

def optimize(w, dw, lr):
    return w + lr * dw

# ────────────────────────────────────────────────
#  Epoch-wise Training and Loss Tracking
# ────────────────────────────────────────────────
def train_svm(X, y, epochs=1601, lr=0.001):
    w = np.zeros(X.shape[1])
    losses = []

    for epoch in range(1, epochs):
        for i in range(len(X)):
            dw = svm_gradient(X[i], y[i], w, epoch)
            w = optimize(w, dw, lr)
        if epoch % 100 == 0:
            loss = hinge_loss_fn(y, forward(X, w)).mean()
            losses.append(loss)
            print(f"Epoch {epoch} - Hinge Loss: {loss:.4f}")
    return w, losses

w_final, losses = train_svm(X, y)

# ────────────────────────────────────────────────
# Visualization: Loss Curve
# ────────────────────────────────────────────────
plt.plot(losses)
plt.title("SVM Training Loss Curve")
plt.xlabel("Epochs (x100)")
plt.ylabel("Hinge Loss")
plt.grid(True)
plt.show()

# ────────────────────────────────────────────────
#  Accuracy Estimation Based on Fraud Prediction
# ────────────────────────────────────────────────
def accuracy(pred, y_true):
    pred_labels = np.where(pred > 0, 1, -1)
    correct = np.sum(pred_labels == y_true)
    return correct / len(y_true)

pred = forward(X, w_final)
acc = accuracy(pred, y)
print(f"Custom SVM Accuracy: {acc:.4f}")

# Ethical Farming
This custom SVM implementation emphasizes algorithmic transparency and control over decision boundaries.

 Manual gradient updates allow inspection of learning behavior  
Hinge loss tracks margin violations for fairness-aware evaluation  
No hidden layers or black-box abstractions—ideal for auditability  
Future enhancements may include kernel methods, SHAP values, and bias mitigation

This approach is especially valuable in civic, legal, and migration contexts where interpretability and trust matter as much as performance.


